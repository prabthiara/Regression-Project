#!/usr/bin/env python
# coding: utf-8

# ## Classification Project

# ### Task:
# Make a prediction for a company on which customers are more likely to purchase or not purchase

# ### Dataset Given:
# We will work with a dataset that contains details about 400 clients of a company including the unique ID, the gender, the age of the customer and the salary. This dataet was obtained from kaggle and can be found in this link: https://www.kaggle.com/datasets/denisadutca/customer-behaviour

# ### Problem:
# Companies want more customers to purchase thier prodects and learning the behavior of their customers can help increase the purchases rate.

# ### Objective/Solution:
# Create a machine learning algorithm that will show which customers are more likely to purchase the companies products and which customers to target.

# ### Dataset information
# - 5 attributes
# - 400 instances

# ### IMPORT LIBRARIES/DATASETS AND PERFORM EXPLORATORY DATA ANALYSIS

# In[75]:


import numpy as np # Multi-dimensional array manipulation
import pandas as pd # DataFrame Manipulation
import matplotlib.pyplot as plt # Data Visualization
import seaborn as sns # Data Visualization
from sklearn import metrics


# In[76]:


# Force Pandas to display all rows and columns
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)


# In[77]:


# Read the CSV file 
df = pd.read_csv('Customer_Behaviour.csv')
df


# In[78]:


# Load the top 5 instances
df.head()


# In[79]:


# Check the shape of the dataframe
df.shape


# In[80]:


# Display the feature columns
df.columns


# In[81]:


# Obtain the summary of the dataframe data types
telecom_df.dtypes


# ### PERFORM DATA VISUALIZATION

# In[82]:


df.hist(figsize = (30, 30))
plt.show()


# In[83]:


# See how many people purchased and did not purchase
df['Purchased'].value_counts()


# In[84]:


# Make a pie chart of the Purchased vs Not Purchased customers
plt.figure(figsize = [10, 10])
df['Purchased'].value_counts().plot(kind='pie')


# In[85]:


# Correlation Matrix
corr_matrix = df.corr()
plt.figure(figsize = (15, 15))
cm = sns.heatmap(corr_matrix,
               linewidths = 1,
               annot = True, 
               fmt = ".2f")
plt.title("Correlation Matrix of Customers", fontsize = 20)
plt.show()

# It is clearly shown that "Age" and "Purchased" are correlated.


# In[86]:


# Plot kde estimate to show
ax = sns.kdeplot(df.Age[(df["Purchased"] == 0)],
               color = "Red", shade = True)
ax = sns.kdeplot(df.Age[(df["Purchased"] == 1)],
               color = "Blue", shade = True)

ax.legend(["No", "Yes"], loc = "upper right")
ax.set_ylabel("Density")
ax.set_xlabel("Age")
ax.set_title("Distribution of Purchased by Age")
# We can see that the customers that did purchase were older than the customers who did not purchase


# ### IDENTIFY FEATURE IMPORTANCE & PREPARE THE DATA BEFORE MODEL TRAINING

# In[87]:


# Fix the 'Gender' column in the dataframe
def gender_type (row):
   if row['Gender'] == 'Male':
      return 1
   if row['Gender'] == 'Female':
      return 2


# In[88]:


# Apply the function now to make a new column that contains integer values for the different values in the 'Gender' column.
df['Gender'] = df.apply (lambda row: gender_type(row), axis=1)


# In[89]:


# Check dataset's head now.
df.head()


# In[90]:


# Unnecessary features would decrease the training speed, the model interpretability and the generalization performance on the test data. 
# Therefore, finding and selecting the most useful features in the dataset is crucial.
# Assigning input features to X and output to y

X = df.drop(["Purchased", "User ID"], axis = "columns")
y = df["Purchased"]


# In[91]:


X.shape


# In[92]:


y.shape


# In[93]:


# Perform train/test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 150)


# In[94]:


X_train.shape


# In[95]:


X_test.shape


# In[96]:


from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train, y_train.values.ravel())


# In[97]:


# Plot the feature importance

feat_scores= pd.DataFrame({"Fraction of variables affected" : rf.feature_importances_},index = X.columns)
feat_scores= feat_scores.sort_values(by = "Fraction of variables affected")
feat_scores.plot(kind = "barh", figsize = (10, 5))
sns.despine()


# - The above graph is generated by Random Forest algorithm 
# - The graph indicates that "EstimatedSalary" tops the list of important features followed by "Age"

# ### TRAIN AND EVALUATE A LOGISTIC REGRESSION CLASSIFIER

# In[98]:


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

model_LR = LogisticRegression()
        
model_LR.fit(X_train, y_train)


# In[99]:


y_predict = model_LR.predict(X_test)


# In[100]:


print(classification_report(y_test, y_predict))
# precision is the ratio of TP/(TP+FP)
# recall is the ratio of TP/(TP+FN)
# F-beta score can be interpreted as a weighted harmonic mean of the precision and recall
# where an F-beta score reaches its best value at 1 and worst score at 0. 
# The model has an accuracy of 72% but did very poorly on test 1.


# ### TRAIN AND EVALUATE A SUPPORT VECTOR MACHINE CLASSIFIER

# In[101]:


from sklearn.calibration import CalibratedClassifierCV # For probability score output
from sklearn.svm import LinearSVC

model_svc = LinearSVC(max_iter=100000)
model_svm = CalibratedClassifierCV(model_svc) 
model_svm.fit(X_train, y_train)


# In[102]:


y_predict = model_svm.predict(X_test)


# In[103]:


print(classification_report(y_test, y_predict))
# The model has an accuracy of 78% but did poorly on test 1 still


# In[104]:


cm = confusion_matrix(y_test, y_predict)
sns.heatmap(cm, annot = True)
# The model got 94 sample right but messed up on 26 samples


# ### TRAIN AND EVALUATE A RANDOM FOREST CLASSIFIER

# In[105]:


from sklearn.ensemble import RandomForestClassifier

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)


# In[106]:


y_predict = model_rf.predict(X_test)


# In[107]:


print(classification_report(y_test, y_predict))
# The model has an accuracy of 93% and did well on both tests


# In[108]:


cm = confusion_matrix(y_test, y_predict)
sns.heatmap(cm, annot = True)
# The model got 112 samples right and messed up on 8 samples


# ### TRAIN AND EVALUATE A K-NEAREST NEIGHBOUR (KNN)

# In[109]:


from sklearn.neighbors import KNeighborsClassifier

model_knn = KNeighborsClassifier()
model_knn.fit(X_train, y_train)


# In[110]:


y_predict = model_knn.predict(X_test) 


# In[111]:


print(classification_report(y_test, y_predict))
# The model has an accuracy of 79% but did not do so great on test 1


# In[112]:


cm = confusion_matrix(y_test, y_predict)
sns.heatmap(cm, annot = True)
# The model got 95 samples right and messed up on 25 samples


# ### TRAIN AND EVALUATE A NAIVE BAYES CLASSIFIER

# In[113]:


from sklearn.naive_bayes import GaussianNB


# In[114]:


model_gnb = GaussianNB()
model_gnb.fit(X_train, y_train)


# In[115]:


y_predict = model_gnb.predict(X_test)


# In[116]:


print(classification_report(y_test, y_predict))
# The model has an accuracy of 92% and did well on both tests


# In[117]:


cm = confusion_matrix(y_test, y_predict)
sns.heatmap(cm, annot = True)
# The model got 110 samples right and messed up on 10 samples.


# ### COMPARE TRAINED CLASSIFIER MODELS AND CONCLUDING REMARKS

# In[118]:


model_LR.predict_proba(X_test)
# The first element is the probability that the output will be 0 
# The second element is the probabiliy that the output will be 1


# In[119]:


model_LR.predict_proba(X_test)[:, 1]


# In[120]:


y_test


# In[121]:


fpr1, tpr1, thresh1 = metrics.roc_curve(y_test, model_LR.predict_proba(X_test)[:, 1], pos_label= 1)


# In[122]:


fpr1


# In[123]:


tpr1


# In[124]:


thresh1


# In[125]:


# ROC curve
from sklearn.metrics import roc_curve

fpr1, tpr1, thresh1 = roc_curve(y_test, model_LR.predict_proba(X_test)[:, 1], pos_label = 1)
fpr2, tpr2, thresh2 = roc_curve(y_test, model_svm.predict_proba(X_test)[:, 1], pos_label = 1)
fpr3, tpr3, thresh3 = roc_curve(y_test, model_rf.predict_proba(X_test)[:, 1], pos_label = 1)
fpr4, tpr4, thresh4 = roc_curve(y_test, model_knn.predict_proba(X_test)[:, 1], pos_label = 1)
fpr5, tpr5, thresh5 = roc_curve(y_test, model_gnb.predict_proba(X_test)[:, 1], pos_label = 1)


# In[126]:


# AUC score

from sklearn.metrics import roc_auc_score

auc_score1 = roc_auc_score(y_test, model_LR.predict_proba(X_test)[:, 1])
auc_score2 = roc_auc_score(y_test, model_svm.predict_proba(X_test)[:, 1])
auc_score3 = roc_auc_score(y_test, model_rf.predict_proba(X_test)[:, 1])
auc_score4 = roc_auc_score(y_test, model_knn.predict_proba(X_test)[:, 1])
auc_score5 = roc_auc_score(y_test, model_gnb.predict_proba(X_test)[:, 1])

print("Logistic Regression: ", auc_score1) # Logistic Regression
print("Support Vector Machine: ", auc_score2) # Support Vector Machine
print("Random Forest: ", auc_score3) # Random Forest
print("K-Nearest Neighbors: ", auc_score4) # K-Nearest Neighbors
print("Naive Bayes: ", auc_score5) # Naive Bayes


# In[127]:


plt.plot(fpr1, tpr1, linestyle = "--", color = "orange", label = "Logistic Regression")
plt.plot(fpr2, tpr2, linestyle = "--", color = "red", label = "SVM")
plt.plot(fpr3, tpr3, linestyle = "--", color = "green", label = "Random Forest")
plt.plot(fpr4, tpr4, linestyle = "--", color = "yellow", label = "KNN")
plt.plot(fpr5, tpr5, linestyle = "--", color = "black", label = "Naive bayes")

plt.title('Receiver Operator Characteristics (ROC)')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')

plt.legend(loc = 'best')
plt.savefig('ROC', dpi = 300)
plt.show()


# The graph represents that Naive Bayes algorithm produced the best AUC. Therefore, it is clear that Naive Bayes model did a better job of classifying the purchased/not purchased customers.

# In[129]:


y_predict = model_gnb.predict(X_test)
print(classification_report(y_test, y_predict))


# Amongst all the trained models, Naive Bayes algorithm produced the highest Area under the ROC curve (AUC).
# 
# The following scores are the results of the Naive Bayes model
# 
# - Accuracy: ~92% label accuracy
# - Precision: ~96% labeled as customers who did not purchase and ~82% labeled as customers who did purchase
# - Recall: ~92% labeled as customers who did not purchase and ~91% labeled as customers who did purchase
